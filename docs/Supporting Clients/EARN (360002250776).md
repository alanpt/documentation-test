# EARN

::: {#readme .Box-body .readme .blob .js-code-block-container .p-5 .p-xl-6 .gist-border-0}
**[Note - EARN has been temporarily disabled through
<https://github.com/waysact/evergiving/issues/8180> because it wasn\'t
working.]{.wysiwyg-color-red}**

**Overview from James**

At Evergiving we are often asked the relative benefit of a particular
strategy for the early retention of newly recruited regular givers. This
makes sense; we operate at the front line of regular giving recruitment
in more than 30 countries and a quarter of the worlds regular givers
come through our platform each year. \"I really want to do payment at
point of sale, but I need a business case for my board. Can you help?\"
The answers are a little embarrassing, as is how long it has taken us to
properly tackle them. I don\'t know exactly. No one knows. Not because
of other variables in recruitment (which is a related challenge), but
because of the enormous variety of methods for measuring retention of
regular givers.

I recently ran a workshop in Sydney for the Australian \'Declines User
Group\'. This group meets regularly to discuss practices that influence
successful regular giving retention. Every country should have such a
group. In Australia it is very well represented by charities that
benefit from regular giving. I was invited by the group to speak, and it
is testament to the exciting field this is and the quality of the group
that my mind boggled with possible topics. I kept coming back to one
vexing problem: How could I present anything we\'re doing to such a
forum, where the currency is the potential to shave percentage points
off attrition, when there is no consistent means of measuring it across
charities and of comparing it to other channels? So I started by running
a survey on how individual charities, most of them global, measure
retention. One of the questions I asked was, whether they could
\'measure it better\'. It was the only question everyone agreed upon.

As an industry we measure attrition on donors being \'active\' at
\'specific points\'. The problem is there is no consensus on the
definitions of either and they vary so wildly that it has become a
problem. Particularly when it comes to the pointy end of regular giving
retention; in the first fragile few months of a donor\'s life where
there is most to gain from innovation and stewardship.

There are half a dozen interpretations of a donor\'s start date, ranging
from the moment a donor is recruited to the date they are imported in to
a donor database, to the date their first payment is scheduled. The span
between these dates are reliably substantial and often measured in
months. It\'s even common practice to exclude cohorts of donors that
drop off immediately or would otherwise negatively impact statistics.
Early retention is as often measured at 3 calendar months as it is 13
weeks or 100 days. The definition of \'active\' is even worse, a
casualty of years of competing contract negotiations redefining the
definitions over and over again. Not to mention constraints of software
and resource. At best it\'s inconsistent, at worst it\'s arbitrary. Face
to face recruitment of regular givers \'has high rates of attrition\'
and by any method this is true. The problem is it is unknown to
precisely what degree. As a result it is used as a stick to beat the
channel with rather than as an objective measure, to be analysed in the
ROI mix.

So why bother? Well, face to face acquired regular givers pledge \$1.2
billion in new annual recurring revenue each year and by anecdotal
measures 45%, or a whopping half a billion dollars, never makes it in to
the bank accounts of the charities it is pledged to. Technology has
created an opportunity for us to not only understand regular giving
better, but provide opportunities for improving retention by large
margins. Even shaving percentage points is measured in the tens of
millions of dollars. Considering the difference this money would make,
it is more important than ever to invest in face to face fundraising,
and specifically in the early retention of enormously valuable new
regular donors. But for this to work we need a new measurement, one that
is consistent and accurate; that allows comparison of initiatives across
the industry and between channels, that is easy for database admins to
generate, and for everyone to understand and use. A universal number. We
call it the Evergiving Attrition and Retention Number (EARN) tm.

How does it work? Multiply number of payments made (money in the bank)
by a frequency normal, then divide by calendar months from date of first
payment attempt and cap to 1.0.

**July 2017:**

For each donor, divide number of payments made (money in the bank) by
calendar months from date of first payment attempt and cap at 1.0. For
non-monthly donors multiply number of payments made by a frequency
normal first: Weekly 12/52 = 0.23 Fortnightly: 12/26 = 0.46 Four weekly
12/13 = 0.92 Quarterly 12/4 = 3 Bi-annual 12/2 = 6 Annual 12/1 = 12 What
you need to know:

1.  The date of first payment attempt (Instant Payments through
    Evergiving will be included as the first payment)
2.  The frequency (if not monthly)
3.  The dates money successfully landed in the bank account.
4.  Nothing more. Everything else is a variable that we use EARN to
    analyse. Why payments? Funds in a bank account are neutral of any
    variable and are unequivocal. There is no clearer measure, nor more
    accurate, nor universal. Why calendar months? They are widely
    adopted and understood measures of reporting. Calculating calendar
    months from the first payment attempt more importantly allows an
    appropriate amount of time for retrying payments or delinquency
    processes. If a payment occurs during the window it is counted. We
    normalise frequency and we cap at 1.0 so as to not overly distort
    results from lower frequencies when combined, 1.0 being a
    \'perfect\' score. What about cancellations? A cancellation should
    not be used to calculate attrition or retention. It is too
    unreliable, too open to interpretation or even abuse (ref contract
    negotiations) if the donor simply stops paying and is uncontactable
    or won\'t be contacted, at some point a human must make a decision
    and mark them \'cancelled\'. Analysing cancellations as variables
    rather than measurements of attrition even allows for interesting
    insights and are an opportunity to better plan resource. The same
    goes for donors \'on-hold\' or that downgrade, or regularly decline
    payments but succeed on retrying. EARN provides a number from 0.0 to
    1.0 for A/B testing or comparing all sorts of variables: dollar
    value of asks; methods of fundraising; retention marketing
    strategies; payment methods; even something as granular as a card
    type or age range. \"What\'s the EARN of a Credit Card vs a Debit
    Card\". \"How does that compare to direct debit in the UK, or
    SEPA\". \"What about at 6 months, or 36?\". \"What\'s the EARN of
    21-35 year olds vs over 65s\". EARN helps me answer the question at
    the top by saying campaigns with instant payments have an EARN value
    at month 3 that is 0.17 greater than campaigns where it takes 21
    days or more from sign up date to first payment date. It\'s then
    simple arithmetic to calculate the benefit of, and therefore the
    precise ROI of setting up a payment gateway. Suppliers of regular
    givers can pitch based on their EARN at a particular month,
    Charities can KPI a minimum EARN, and agencies can demand the same
    from their staff. We can compare face to face to digital, to
    telemarketing, to direct mail, and share attributes of higher EARNs.
:::
